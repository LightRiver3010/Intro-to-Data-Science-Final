import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

df = pd.read_csv('./data/Student Depression Dataset.csv')
df.head()

# making a cleaned-up dataset
df1 = df

# importing the models and splitting methods
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

df.columns

# changing Gender column to fix model
# Male == 0, Female = 1
df1['Gender'].value_counts()
def gender_converter(args):
    if args == 'Male':
        return 0
    elif args == 'Female':
        return 1
    else:
        return args
df1["Gender"] = df1["Gender"].apply(gender_converter)
df1.head()

# renaming columns
df1 = df1.rename(columns = {"Have you ever had suicidal thoughts ?": "Suicidal throughts present?", 
"Family History of Mental Illness" : "Family History of Mental Illness?"})
df1.head()

#changing values to 0 and 1
def boolean_changer(args):
    if args == "Yes":
        return 1
    elif args == "No":
        return 0
    else:
        return args
df1["Suicidal throughts present?"] = df1["Suicidal throughts present?"].apply(boolean_changer)
df1["Family History of Mental Illness?"] = df1["Family History of Mental Illness?"].apply(boolean_changer)
df1.head()

# fixing 'Sleep Duration' column
df1 = df1[df1['Sleep Duration'] != 'Others']
df1['Sleep Duration'].value_counts()

def sleep_converter(args):
    if args == "7-8 hours":
        return 3
    elif args == "5-6 hours":
        return 2
    elif args == 'Less than 5 hours':
        return 1
    elif args == 'More than 8 hours':
        return 4
df1["Sleep Duration"] = df1["Sleep Duration"].apply(sleep_converter)
df1.head()

# Fixing Dietary Habits
df1 = df1[df1['Dietary Habits'] != 'Others']
df1['Dietary Habits'].value_counts()

# Better diet == higher int value
def dietary_changer(args):
    if args == "Healthy":
        return 3
    elif args == "Moderate":
        return 2
    elif args == 'Unhealthy':
        return 1
    else:
        return args
df1["Dietary Habits"] = df1["Dietary Habits"].apply(dietary_changer)
df1.head()

df1['Job Satisfaction'].value_counts() # job satisfaction has little to no variance so it's getting dropped lol
df1 = df1.drop('Job Satisfaction', axis = 1)
df1.head()

df1['Work Pressure'].value_counts() # also has lack of variance
df1 = df1.drop('Work Pressure', axis = 1)
df1.head()

df1['Study Satisfaction'].value_counts() # has variance, so it can stay :)

#fixing the cities column
df1['City'].value_counts() # has variance so we can't disregard it

# filter out the cities with low populations/are outliers
df1 = df1[df1['City'].map(df1['City'].value_counts()) > 2]
df1['City'].value_counts()

# creating a column that organizes each city into a state
def state_converter(args):
    if args == 'Srinagar':
        return "Jammu and Kashmir"
    elif args == 'Hyderabad':
        return "Telagana"
    elif (args == "Vasai-Virar") or (args == "Thane") or (args == "Pune") or (args == "Mumbai") or (args == "Nagpur") or (args == "Nashik"):
        return "Maharashtra"
    elif (args == "Lucknow") or (args == "Agra") or (args == "Meerut") or (args == "Ghaziabad") or (args == "Vadodara") or (args == "Varanasi") or (args == "Kanpur"):
        return "Uttar Pradesh"
    elif args == "Ludhiana":
        return "Punjab"
    elif (args == "Surat") or (args == "Ahmedabad") or (args == "Rajkot") or (args == "Vadodara"):
        return "Gujarat"
    elif args == "Kolkata":
        return "West Bengal"
    elif args == "Jaipur":
        return "Rajasthan"
    elif args == "Patna":
        return "Bihar"
    elif args == "Visakhapatnam":
        return "Andhra Pradesh"
    elif (args == "Bhopal") or (args == "Indore"):
        return "Madhya Pradesh"
    elif args == "Chennai":
        return "Tamil Nadu"
    elif args == "Delhi":
        return "NCT of Delhi"
    elif args == "Bangalore":
        return "Karnataka"
    elif args == "Faridabad":
        return "Haryana"
df1["State"] = df1["City"].apply(state_converter)
df1['State'].value_counts()

# dividing each state into respective regions
def region_converter(args):
    if (args == 'Uttar Pradesh')   or (args == 'Madhya Pradesh'):
        return 'Central'
    elif (args == 'Uttar Pradesh') or (args == 'Telagana') or (args == 'Andhra Pradesh') or (args == 'Tamil Nadu') or (args == 'Karnataka'):
        return 'Southern'
    elif (args == 'Jammu and Kashmir') or (args == 'Punjab') or (args == 'Rajasthan') or (args == 'NCT of Delhi') or (args == 'Haryana'):
        return 'Northern'
    elif (args == 'West Bengal') or (args == 'Bihar'):
        return 'Eastern'
    elif (args == 'Maharashtra') or (args == 'Gujarat'):
        return 'Western'
    else:
        return "bruh"
df_processed["Region"] = df_processed["State"].apply(region_converter)
df_processed['Region'].value_counts()

# making the regions numerical
def region_converter(args):
    if (args == 'Uttar Pradesh')   or (args == 'Madhya Pradesh'): # Central
        return '1'
    elif (args == 'Uttar Pradesh') or (args == 'Telagana') or (args == 'Andhra Pradesh') or (args == 'Tamil Nadu') or (args == 'Karnataka'): # Southern
        return '2'
    elif (args == 'Jammu and Kashmir') or (args == 'Punjab') or (args == 'Rajasthan') or (args == 'NCT of Delhi') or (args == 'Haryana'): # Northern
        return '4'
    elif (args == 'West Bengal') or (args == 'Bihar'): # Eastern
        return '3'
    elif (args == 'Maharashtra') or (args == 'Gujarat'): # Western
        return '5'
df_processed["Region"] = df_processed["State"].apply(region_converter)
df_processed['Region'].value_counts()

df_processed['Degree'].value_counts() # has variance

## Creating Machine Learning Model

# Seeeing the most accurate model
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report,confusion_matrix

# removing non-numerical values for the machine learning
df_processed = df_processed.drop(['City', 'id', 'Profession', 'Degree', 'State'], axis = 1)
df_processed.dropna(inplace = True)

# creating data splits
X = df_processed.drop('Depression',axis=1)
Y = df_processed['Depression']
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=1, shuffle = True)
models = []
models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('RFR', RandomForestClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC(gamma='auto')))
# evaluate each model in turn
results = []
names = []
for name, model in models:
    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)
    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')
    results.append(cv_results)
    names.append(name)
    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))
# Compare Algorithms
pyplot.boxplot(results, labels=names)
pyplot.title('Algorithm Comparison')
pyplot.show() #SVM has the highest accuracy, so we will be using that model

## Hyperoptimization
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV

log_model = LogisticRegression()
log_model.fit(X_train,Y_train)

# Define the parameter values to be searched
param_grid = [
    {'penalty':['l1','l2','elasticnet'],
    'solver': ['lbfgs','newton-cg','liblinear','sag','saga'],
    'max_iter'  : [100,1000,2500,5000]
}
]

# Instantiate the grid
grid_search = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv=5)

# Fit the grid with data
grid_search.fit(X_train, Y_train)

# Printing the best parameters
print("Best parameters:", grid_search.best_params_)

log = LogisticRegression(penalty = 'l1', solver = 'saga', max_iter = 1000)

log.fit(X_train,Y_train)
predictions = log.predict(X_test)
print(log.score(X_test, Y_test))
cv_results = cross_val_score(log, X_train, Y_train, cv=kfold, scoring='accuracy')
print(cv_results.mean())

##Making Graphs
import seaborn as sns

#heatmap of data
plt.figure(figsize=(12, 8))  # Set the figure size
df_corr = df_processed.corr()
sns.heatmap(df_corr, annot=True).set(title='Depression Heatmap')
plt.show()

# ROC Curve
y_pred_proba = log.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(Y_test,  y_pred_proba)
auc = metrics.roc_auc_score(Y_test, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.title('ROC Values')
plt.show()

gender = int(input('Enter your gender (0 = male, 1 = female):'))
print('Gender = ', gender)
age = int(input('Enter your age:'))
print('Age = ', age)
aca_pressure = int(input('Enter your Academic Pressure (0-5):'))
print('aca_pressure = ', aca_pressure)
cgpa = int(input('Enter your CGPA (0-10):'))
print('CGPA = ', cgpa)
study_satis = int(input('Enter your study satisfaction (0-5):'))
print('Study Satisfaction = ', study_satis)
sleep = int(input('Enter your sleep duration rating (1-4):'))
print('Sleep Duration = ', sleep)
diet = int(input('Enter your dietary habits (1-3):'))
print('Dietary Habits= ', diet)
suicidal_thoughts = int(input('Do you have suicidal thoughts present? (0=no, 1 = yes):'))
print('Suicidal Thoughts Present? = ', suicidal_thoughts)
work_study = int(input('Work/Study Hours Daily:'))
print('Work/Study Hours = ', work_study)
financial = int(input('Financial Stress levels (1-5):'))
print('Financial Stress Level = ', financial)
fam_history = int(input('Family History of Mental Illness? (0 = no, 1 = yes): '))
print('Family History of Mental Illness? = ', fam_history)
region = 1

stats = [gender, age, aca_pressure, cgpa, study_satis, sleep, diet, suicidal_thoughts, work_study, financial, fam_history, 1]
# predict the class label

yhat = log.predict([stats])
# summarize the predicted class
print('Predicted Class: %d' % yhat[0])
